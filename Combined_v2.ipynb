{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2f760f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e71f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphamap.importing import import_data\n",
    "from alphamap.preprocessing import format_input_data\n",
    "from pyteomics import fasta\n",
    "\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from statistics import mean,stdev,mode\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76492051",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 'Z:\\Helium_Tan\\R02_PTMDIA\\Pro\\DIANN\\Combined_DDA_Predicted_Library\\dia_nn\\out' + '\\\\'\n",
    "with open(wd + 'input.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "17014717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51 DIANN\n"
     ]
    }
   ],
   "source": [
    "#Unpack yaml file input parameters\n",
    "file_name = params['main']['report']\n",
    "fasta_path = params['main']['fasta_path']   #File path that points directly to the file\n",
    "engine = params['main']['engine']\n",
    "\n",
    "target_ptm = params[engine]['target_ptm']\n",
    "experimental = params[engine]['experimental']\n",
    "heavies = params[engine]['heavies'] \n",
    "file_col = params[engine]['file']\n",
    "all_sequences = params[engine]['all_sequences']\n",
    "prot_column = params[engine]['prot_column']\n",
    "\n",
    "ptm_phrase = params['phrases'][target_ptm]\n",
    "threshold = params[engine]['ptm_confidence_threshold']\n",
    "\n",
    "print(threshold, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fad0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIANN Pre-processing#\n",
    "def preprocess(report):\n",
    "    \"\"\"\n",
    "    Localizes report for site confidence >= threshold, exports localized report. Performs alphamap processing of report to get peptide position and PTM information. Adjusts PTM annotations.\n",
    "\n",
    "    Args: Long-form DIA-NN report\n",
    "\n",
    "    Returns: (Localized dataframe report, alphamap output) for further processing of DIA-NN data\n",
    "    \"\"\"\n",
    "    #Localize report, export localized report\n",
    "    if threshold != 0:\n",
    "        localized_report = report[report['PTM.Site.Confidence'] >= threshold]\n",
    "        localized_report.to_csv(wd + 'Localized_Report.tsv', sep = '\\t', index = False)\n",
    "        report_path = wd + 'Localized_Report.tsv'\n",
    "        \n",
    "    else: \n",
    "        localized_report = report\n",
    "        report_path = wd + file_name\n",
    "        \n",
    "\n",
    "    print('Report with confidently localized entries exported.')\n",
    "\n",
    "    #Prepare report and fasta for Alphamap processing\n",
    "    diann_report = import_data(report_path)\n",
    "    fasta_file = fasta.IndexedUniProt(fasta_path)\n",
    "    print('Prepared for alphamap processing')\n",
    "\n",
    "    #Remove contaminants and heavy annotations (does not remove full sequences containing heavies)\n",
    "    diann_report = diann_report[diann_report[\"all_protein_ids\"].str.contains(\"contaminant\") == False].reset_index(drop=True)\n",
    "    diann_report = diann_report.replace('\\(UniMod:259\\)','', regex = True)\n",
    "    diann_report = diann_report.replace('\\(UniMod:267\\)','', regex = True)\n",
    "\n",
    "    #Run report throught alphamap to get formatted data with peptide locations\n",
    "    formatted_data = format_input_data(df=diann_report, fasta=fasta_file, modification_exp = r'\\[.*?\\]')  # Run through alphamap to get peptide locations on protein\n",
    "    formatted_data.rename(columns={'modified_sequence': 'Modified.Sequence', 'all_protein_ids': \"Protein.Ids\"}, inplace=True)   #Rename 'all_protein_ids' column to 'Protein IDs to match the matrix it will be merged with\n",
    "\n",
    "    #Update peptide start and end positions to be 1-indexed instead of 0-indexed\n",
    "    formatted_data['start'] = formatted_data['start'] + 1\n",
    "    formatted_data['end'] = formatted_data['end'] + 1\n",
    "\n",
    "    #Change annotations in the formatted data to match matrix it will be merged with\n",
    "    formatted_data = formatted_data.replace('\\[Phospho \\(STY\\)\\]','(UniMod:21)', regex=True)\n",
    "    formatted_data = formatted_data.replace('\\[Carbamidomethyl \\(C\\)\\]','(UniMod:4)', regex=True)\n",
    "    formatted_data = formatted_data.replace('\\[Acetyl \\(N-term\\)\\]','(UniMod:1)', regex=True)\n",
    "    formatted_data = formatted_data.replace('\\[Oxidation \\(M\\)\\]','(UniMod:35)', regex=True)\n",
    "\n",
    "    formatted_data = formatted_data.sort_values('unique_protein_id')        #Sort unique protein IDs so when STY locations get appended, they match order of alphabetized protein IDs\n",
    "    \n",
    "    return(localized_report, formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6791198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matrix(localized_report, matrix): #DIA-NN\n",
    "    \"\"\"\n",
    "    Filters DIA-NN matrix file-specific quant for sequences that are only in the localized report.\n",
    "    \n",
    "    Args: Localized report, pre-collapsed matrix\n",
    "    \n",
    "    Returns: Matrix where intensities for each sample are only reported on file-specific localized sequences.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    localized = {}\n",
    "    for s in samples:           \n",
    "        one = localized_report[localized_report['File.Name'] == s]   #Only entries from that run in report\n",
    "        if s not in localized:                   #Key is sample name, list of unique modified sequences that are localized in that run\n",
    "            localized[s] = None\n",
    "            localized[s] = one['Modified.Sequence'].unique()\n",
    "            \n",
    "        else:\n",
    "            localized[s] = one['Modified.Sequence'].unique()\n",
    "            \n",
    "    print('Found localized')\n",
    "            \n",
    "    for index, row in matrix.iterrows():          #Only iterate through matrix once\n",
    "        seq = row['Modified.Sequence']\n",
    "        \n",
    "        for s in samples:         \n",
    "            if seq not in localized[s]:\n",
    "                matrix.at[index,s] = None         #If the sequence in the matrix is not localized, replace intensity with None\n",
    "                \n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9b9c10e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DIA-NN report through alphamap library\n",
      "Report with confidently localized entries exported.\n",
      "Import DIA-NN output\n",
      "Prepared for alphamap processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tvashist\\alphamap\\alphamap\\alphamap\\preprocessing.py:65: UserWarning: No matching entry for B99907 in the selected fasta.\n",
      "  warnings.warn(f'No matching entry for {prot} in the selected fasta.')\n",
      "c:\\users\\tvashist\\alphamap\\alphamap\\alphamap\\preprocessing.py:65: UserWarning: No matching entry for B99908 in the selected fasta.\n",
      "  warnings.warn(f'No matching entry for {prot} in the selected fasta.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found localized\n",
      "Ready for processing.\n"
     ]
    }
   ],
   "source": [
    "if engine == 'SN':\n",
    "    report = pd.read_csv(wd + file_name, delimiter ='\\t', low_memory= False)        #Read in Spectronaut \"Normal Report\"\n",
    "    if threshold != 0:\n",
    "        report = report.loc[report['EG.PTMAssayProbability'] >= threshold]\n",
    "\n",
    "\n",
    "    samples = (report[file_col]).unique()                                       #List of unique samples/ LC-MS runs that IDs may come from\n",
    "\n",
    "    \n",
    "elif engine == 'DIANN':\n",
    "    report = pd.read_csv(wd+ file_name, sep = '\\t')\n",
    "    matrix = pd.read_csv(wd + 'report.pr_matrix.tsv', sep = '\\t', low_memory = False)\n",
    "    samples = (report[file_col]).unique()\n",
    "    \n",
    "    print('Running DIA-NN report through alphamap library')\n",
    "    processed = preprocess(report)\n",
    "    localized_report = processed[0]\n",
    "    formatted_data = processed[1]\n",
    "    \n",
    "    filtered_matrix = filter_matrix(localized_report, matrix)\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"Sorry, please check the spelling of the software.\")\n",
    "    \n",
    "print('Ready for processing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917410e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_experimental_mods(sequence):   #SN and DIA-NN\n",
    "    \"\"\"\n",
    "    Removes annotations relating to N-term acetylation, oxidation of methionine, and carbamidomethylation. Also removes '_' flanking sequences to ease indexing in Spectronaut.\n",
    "\n",
    "    Args:\n",
    "        sequence: Sequence containing experimentally-introduced mod annotations listed above.\n",
    "\n",
    "    Returns: Peptide sequence without experimentally-introduced mod annotations listed above.\n",
    "    \"\"\"\n",
    "    for item in experimental:\n",
    "        sequence = sequence.replace(item, '') #Remove carbamidomethylation, N-term acetylation, M-oxidation\n",
    "        \n",
    "    return(sequence)\n",
    "\n",
    "\n",
    "# remove_experimental_mods('_EGPTDHLESAC[+57]PL[+8]NLPLQNNHTAADM[+16]YLS[+80]PVR[+10]S[+80]PK_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f093b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_collapsed_report_SN(report):\n",
    "    \"\"\"\n",
    "    Takes normal report from Spectronaut output and generates collapsed PTM report where each entry represents a unique phosphosite. A phosphosite\n",
    "    is defined as a uniquely phosphorylated peptide sequence. Differentially heavy-modified peptides are listed as unique phosphosite entries.\n",
    "\n",
    "    Args:\n",
    "        report: Spectronaut Normal Report. Must contain the following columns for script to function:\n",
    "            'R.FileName'\n",
    "            'FG.IntMID'\n",
    "            'EG.IntPIMID'\n",
    "            'PG.ProteinAccessions'\n",
    "            'EG.ProteinPTMLocations'\n",
    "            'FG.Quantity'\n",
    "            'EG.PTMAssayProbability'\n",
    "            'EG.Cscore'\n",
    "\n",
    "    Returns: Collapsed PTM site report.\n",
    "    \"\"\"\n",
    "\n",
    "    target = report.loc[report['EG.ModifiedSequence'].str.contains(ptm_phrase)]    #Only entries with target PTM are filtered\n",
    "\n",
    "    \n",
    "    collapse_keys = []\n",
    "\n",
    "    targets_unannotated = []\n",
    "    for index, row in target.iterrows():\n",
    "        \n",
    "        sequence = remove_experimental_mods(row['FG.IntMID'])  \n",
    "        pep_prot_positions = row['PEP.PeptidePosition']\n",
    "        protein_ids = row['PG.ProteinAccessions']\n",
    "        \n",
    "        #Target and heavy PTM positions\n",
    "        ptm_prot_positions = find_prot_positions(sequence, pep_prot_positions)\n",
    "        \n",
    "        #Integer-unannotated target positions used by find_flanking function\n",
    "        unannotated = [[re.sub(\"\\[.*?\\]\", \"\", s) for s in pos if target_ptm in s] for pos in ptm_prot_positions]  #Keeps only sequences with target mod (ex: '+80'), and removes integer annotations in brackets\n",
    "        targets_unannotated.append(unannotated)\n",
    "        \n",
    "        #Collapse keys\n",
    "        k = (protein_ids,str(ptm_prot_positions))    #Collapse key contains: Protein IDs, target PTM protein locations (this way even sequences with other modifications will still collapse if they have the same target PTM positions), heavy mod protein locations\n",
    "        collapse_keys.append(k)\n",
    "\n",
    "        \n",
    "    #Add columns to target dataframe\n",
    "    target['Collapse'] = collapse_keys                                     #Creates new column with collapse key values\n",
    "    target[ptm_phrase +'ProteinLocations'] = targets_unannotated           #Creates new column with integer-unannotated target PTM protein positions\n",
    "    \n",
    "    first_column = target.pop('Collapse')\n",
    "    target.insert(0, 'Collapse', first_column)\n",
    "    \n",
    "#     target.to_csv(wd + 'Keyed_PreCollapse.tsv', sep = '\\t', index = False)\n",
    "    \n",
    "    #In this section, each group under a collapse key is further split into members associated with each individual sample, then quant is separately summed per run within the gorup and added to a dictionary\n",
    "    #Each collapse key is a key in a dictionary where the values\n",
    "\n",
    "    groups = target.groupby('Collapse')   #Group entries by collapse key\n",
    "    \n",
    "    #Structure: {Collapse_Key: {Sample1: Summed_quant_01, Sample2: Summed_quant_02}}\n",
    "\n",
    "\n",
    "    meta = {}\n",
    "    for n, group in groups:\n",
    "        \n",
    "        meta[n] = None              #n is collapse key\n",
    "\n",
    "        file_quant = {}\n",
    "\n",
    "        for s in samples:           #Sum quant by file\n",
    "            file_quant[s] = None\n",
    "\n",
    "            sub = group[group['R.FileName'] == s]       #All entries from that individual run/file/sample\n",
    "            if len(sub) > 0:\n",
    "                quant = sub['FG.Quantity'].sum()              #Sum quant\n",
    "            else:\n",
    "                quant = None                                  #Otherwise append None\n",
    "\n",
    "            file_quant[s] = quant\n",
    "\n",
    "        meta[n] = file_quant                                  #Adding quant dict for that collapse key\n",
    "\n",
    "    #In the following section, rows are being collapsed\n",
    "    \n",
    "    #Only keep row from a group that contains the highest PTM Assay Probability Score (most confident localization); There will be repeats for rows with same PTM Assay Probability Score\n",
    "    group_MaxPTMAssayScore = target.groupby(['Collapse'])['EG.PTMAssayProbability'].transform(max) == target['EG.PTMAssayProbability']\n",
    "    collapsed_MaxPTMAssayScore = target[group_MaxPTMAssayScore]\n",
    "\n",
    "    #Make collapse key first column, will remove this helper column for final output\n",
    "    first_column = collapsed_MaxPTMAssayScore.pop('Collapse')\n",
    "    collapsed_MaxPTMAssayScore.insert(0, 'Collapse', first_column)\n",
    "\n",
    "    #Only keep row from group with highest EG.Cscore; There will be no more repeats\n",
    "    group_MaxCScore = collapsed_MaxPTMAssayScore.groupby(['Collapse'])['EG.Cscore'].transform(max) == collapsed_MaxPTMAssayScore['EG.Cscore']\n",
    "    collapsed_MaxCScore = collapsed_MaxPTMAssayScore[group_MaxCScore]\n",
    "\n",
    "\n",
    "    for f in samples:\n",
    "        collapsed_MaxCScore[f] = [None for x in range(0,len(collapsed_MaxCScore))]      #Create new exmpty column for quant by sample\n",
    "\n",
    "\n",
    "    for index, row in collapsed_MaxCScore.iterrows():          #Iterate through each entry\n",
    "        key = row['Collapse']\n",
    "\n",
    "\n",
    "        for f in samples:                                       #Add quant for this row based on sample it's coming from\n",
    "            q = meta[key][f]\n",
    "            collapsed_MaxCScore.at[index ,f] = q\n",
    "\n",
    "    #Select columns to keep in the ouput\n",
    "    keep = ['Collapse','R.Condition','R.FileName', 'R.Replicate', 'PG.Genes','PG.Organisms', 'PG.ProteinAccessions', 'PG.IBAQ', 'PG.Quantity', 'PEP.PeptidePosition', ptm_phrase +'ProteinLocations', 'PEP.Quantity', 'EG.IntPIMID', 'EG.ProteinPTMLocations',\n",
    "            'EG.PTMAssayProbability','EG.PTMLocalizationProbabilities','EG.Cscore','EG.IntCorrScore','FG.Charge','FG.IntMID','FG.CalibratedMassAccuracy (PPM)']\n",
    "\n",
    "    #Append file ptm_phrases as column titles, these columns contain file-specific quant values\n",
    "    keep.extend(samples)\n",
    "\n",
    "    #Generate new collapsed report with selected columns only\n",
    "    collapsed_report = collapsed_MaxCScore[keep]\n",
    "    collapsed_report.to_csv(wd + 'VM_CollapsedReport.tsv', sep='\\t', index=False)\n",
    "\n",
    "    return(collapsed_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96327bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prot_positions(sequence,prot_positions):\n",
    "    \n",
    "    all = list(heavies)\n",
    "    all.append(target_ptm)\n",
    "\n",
    "    if engine == 'SN':\n",
    "        all_regex = '[A-Z]('+ '|'.join(['\\\\[\\\\' + x + '\\\\]' for x in all]) + ')'        #Generates regex expression that searches for all target ptm and heavy annotations and the amino acid\n",
    "        prot_positions = re.split(r';|,', prot_positions)\n",
    "    \n",
    "    if engine == 'DIANN':\n",
    "        all_regex = '[A-Z]('+ '|'.join(['\\\\(' + x + '\\\\)' for x in all]) + ')'\n",
    "\n",
    "    finder = re.finditer(all_regex, sequence)                  #Find matches within sequence\n",
    "    naked = re.sub(all_regex, 'X', sequence)                   #Replace all matches with a single character to find their position in naked sequence\n",
    "\n",
    "    annotations = [match.group() for match in finder]                               #All annotations with amino acid residue\n",
    "    pep_positions = [match.start() for match in re.finditer('X', naked)]            #Peptide positions of matches (0-indexed)\n",
    "        \n",
    "    full = []\n",
    "    \n",
    "    for p in prot_positions:                                             #PEP_positions relating to different protein IDs are separated by ';'\n",
    "        \n",
    "        protein_pos = [str(int(p) + x) for x in pep_positions]                      #Add the peptide position of the match to the protein position of peptide --> protein position of the match\n",
    "        ann = [''.join(item) for item in zip(*[annotations, protein_pos])]          #Combine the annotation (regex match) with the updated protein position\n",
    "        full.append(ann)\n",
    "        \n",
    "    return(full)\n",
    "        \n",
    "# find_prot_positions('AAVSHWQQQS(UniMod:21)YLDSGIHSGATTTAPSLSGK(UniMod:259)', [40])\n",
    "# find_prot_positions('TAGTSFMMT[+80]PYVVT[+80]R[+10]', '175;68;175;175')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd20601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_formatted_data(formatted_data):  #DIA-NN   \n",
    "    \"\"\"\n",
    "    Adds PTM residue protein positions and condenses alphamap output such that PTM protein locations are given for all protein IDs a sequence maps to in a single row.\n",
    "    \n",
    "    Args: Alphamap output where each entry corresponds to a unique protein ID a sequence maps to.\n",
    "    \n",
    "    Returns: Formatted output collapsed with PTM protein locations for all protein IDs a sequence maps to.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #The Alphamap report creates individual entry for every protein ID a sequence is mapped to, here we group by 'Protein.ids' (all protein IDs a sequence maps to) and 'Modified Sequence'\n",
    "    condense = ['start','end']\n",
    "    list_cols = {col: lambda x: list(x) for col in condense}               #Condenses entries from individual rows to a list of PTM protein locations\n",
    "    \n",
    "    keep = ['Protein.Ids','Modified.Sequence','naked_sequence','PTMtypes','PTMsites']\n",
    "    other_cols = {col: 'first' for col in keep}                                                           #Keeps first of all other entries because they are identical across all rows of the same group\n",
    "\n",
    "    col_map = {**other_cols, **list_cols}                                                                 #Determines how grouping should be done\n",
    "\n",
    "\n",
    "    formatted_data = formatted_data.groupby(['Protein.Ids','Modified.Sequence']).agg(col_map).reset_index(drop=True)\n",
    "    \n",
    "    return(formatted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025f7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabetize(proteins):  #DIA-NN\n",
    "    \"\"\"\n",
    "    Alphabetizes order of protein IDs in DIA-NN matrix 'Protein.Ids' column. This is needed for the merging of the alphamap output to the matrix.\n",
    "\n",
    "    Args: string of protein IDs separated by ';'\n",
    "\n",
    "    Returns: string of alphabetized protein IDs separated by ';'\n",
    "    \"\"\"\n",
    "    ls = sorted(proteins.split(';'))    #Split and sort\n",
    "    str = ';'.join(ls)                  #Rejoin with semicolon into str\n",
    "\n",
    "    return(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc4e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(formatted_data, filtered_matrix):  #DIA-NN\n",
    "    \"\"\"\n",
    "    Merges alphamap output with protein positions to the filtered intensity matrix, which does not have protein positions. \n",
    "    \n",
    "    Args: Formatted alphamap output, DIA-NN matrix output\n",
    "    \n",
    "    Returns: Merged ouput of intensity matrix with protein position information appended.\n",
    "\n",
    "    \"\"\"\n",
    "    print('Merging alphamap output and DIA_NN Precursor matrix')\n",
    "\n",
    "    filtered_matrix['Precursors_Heavies'] = filtered_matrix['Modified.Sequence']       #Maintains record or heavies included in sequence\n",
    "    \n",
    "    for h in heavies:\n",
    "        h = '(' + h + ')'\n",
    "        filtered_matrix['Modified.Sequence'] = filtered_matrix.apply(lambda x: x['Modified.Sequence'].replace(h,''),axis=1) #Removes heavy annotations from this sequence so that it will match the sequences in alphamap output (which cannot contain heavy annotations)\n",
    "        \n",
    "    filtered_matrix['Protein.Ids'] = filtered_matrix.apply(lambda x: alphabetize(x['Protein.Ids']),axis=1)  # Alphabetizes protein IDs in matrix so\n",
    "    \n",
    "\n",
    "    #Make changes to alphamap output\n",
    "    formatted_data = condense_formatted_data(formatted_data)                           #Adds PTM protein locations to formatted data\n",
    "\n",
    "    new_df = filtered_matrix.merge(formatted_data, \"inner\", on=[\"Modified.Sequence\", \"Protein.Ids\"])     #Merges two data frames based on common protein IDs and modified sequences that do not contain heavies\n",
    "    new_df.to_csv(wd + 'Merged.tsv', sep = '\\t')\n",
    "\n",
    "    return(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d2c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_collapsed_report_DIANN(formatted_data, filtered_matrix):\n",
    "    \"\"\"\n",
    "    Generates collapsed PTM report where each entry represents a unique phosphosite. A phosphosite\n",
    "    is defined as a uniquely phosphorylated peptide sequence. Differentially heavy-modified peptides are listed as unique phosphosite entries.\n",
    "\n",
    "    Args:\n",
    "        formatted_data = alphamap output\n",
    "        filtered_matrix = Filtered DIA_NN precursor matrix\n",
    "\n",
    "\n",
    "    Returns: Collapsed PTM site report. Additional columns added:\n",
    "        'Only_Target_Mods': Peptide sequence with only target PTM annotated\n",
    "        'HeavyMod_ProteinPositions': Protein location of heavy-labeled amino acids\n",
    "\n",
    "    \"\"\"\n",
    "    report = merge(formatted_data, filtered_matrix)                                      #Generate merged dataframe that will then be collapsed\n",
    "\n",
    "    target = report.loc[report['Modified.Sequence'].str.contains(target_ptm)]            #Only phospho\n",
    "    \n",
    "\n",
    "    collapse_keys = []\n",
    "    targets_unannotated = []\n",
    "\n",
    "    for index, row in target.iterrows():\n",
    "        protein_ids = row['Protein.Ids']\n",
    "        \n",
    "        sequence = remove_experimental_mods(row['Precursors_Heavies'])                    #This column contains sequences containing heavy mods\n",
    "        pep_prot_positions = row['start']\n",
    "        \n",
    "        #Target and heavy PTM positions\n",
    "        ptm_prot_positions = find_prot_positions(sequence, pep_prot_positions)\n",
    "        \n",
    "        #Integer-unannotated target positions used by find_flanking function\n",
    "        unannotated = [[re.sub(\"\\(.*?\\)\", \"\", s) for s in pos if target_ptm in s] for pos in ptm_prot_positions]  #Keeps only sequences with target mod (ex: '+80'), and removes integer annotations in brackets\n",
    "        targets_unannotated.append(unannotated)\n",
    "\n",
    "        #Generate collapse key: #Collapse key contains: Protein IDs, PTM protein locations, heavy mod protein locations\n",
    "        k = (protein_ids,str(ptm_prot_positions))         \n",
    "        collapse_keys.append(k)\n",
    "\n",
    "    target['Collapse'] = collapse_keys                                     #Creates new column with collapse key values\n",
    "    target[ptm_phrase +'ProteinLocations'] = targets_unannotated\n",
    "\n",
    "    #Prepare columns for grouping\n",
    "    keep = ['Collapse', 'Protein.Group', 'Protein.Ids', 'Protein.Names', 'Genes', 'First.Protein.Description',\n",
    "            'Proteotypic', 'Stripped.Sequence', 'Modified.Sequence', 'Precursors_Heavies','Precursor.Id', 'start', 'end',\n",
    "            'PTMsites', 'PTMtypes', ptm_phrase+'ProteinLocations']\n",
    "\n",
    "    other_cols = {col: 'first' for col in keep}\n",
    "    quants = {col: lambda x: x.sum(skipna=False) for col in samples}       #Sum all the values that are not NA\n",
    "\n",
    "    col_map = {**other_cols, **quants}\n",
    "\n",
    "    collapsed = target.groupby(['Collapse'], as_index=False).agg(col_map)  #Group by collapse key, sum values by sample\n",
    "\n",
    "\n",
    "    # #Append file names as column titles, these columns contain file-specific quant values\n",
    "    keep.extend(samples)\n",
    "\n",
    "    collapsed_report = collapsed[keep]\n",
    "#     collapsed_report.to_csv(wd + 'VM_CollapsedReport.tsv', sep = '\\t', index = False )\n",
    "    \n",
    "    return(report, collapsed_report)  #Return merged matrix and collapsed report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3194942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_collapsed_report():\n",
    "    if engine == 'SN':\n",
    "        collapsed_report = generate_collapsed_report_SN(report)\n",
    "        \n",
    "    if engine == 'DIANN':\n",
    "        collapsed_report = generate_collapsed_report_DIANN(formatted_data, filtered_matrix)[1]\n",
    "        \n",
    "    return(collapsed_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3b4e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_non_missing(setlist, collapsed_report, samples, sample, threshold):\n",
    "    \"\"\"\n",
    "    Reports summary stats of sequences present across all samples searched together.\n",
    "\n",
    "    Args:\n",
    "        setlist: list of sets, one set of sequences per sample\n",
    "        collapsed_report: Collapsed PTM site report\n",
    "        samples: List of all names\n",
    "        sample: Column name that will be used for the summary document\n",
    "\n",
    "    Returns: Dictionary with values for\n",
    "        % precursors containing target modification \n",
    "        Number of modified peptides: Including different versions of experimentally-introduced modifications and heavy mods\n",
    "        Number of phosphopeptides: Number of unique PTM sequences, NOT including experimentally-introduced modifications, but including heavy mods\n",
    "        Number of phosphosites: Collapsed PTM-sites, including heavy mods\n",
    "\n",
    "    \"\"\"\n",
    "    intersection = set.intersection(*setlist)                                 #Sequences that are found across all samples\n",
    "    num_modified_sequences = len(intersection)\n",
    "\n",
    "    ptm_sequences = [x for x in intersection if target_ptm in x]              #Only PTM-containing precursors\n",
    "    enrichment = (len(set(ptm_sequences)) / num_modified_sequences) * 100     #PTM-containing precursors / all precursors\n",
    "\n",
    "    ptm_no_experimental = list(map(remove_experimental_mods, ptm_sequences))  #Remove experimentally-introduced modifications, keep heavy mods\n",
    "    unique_ptm_peptides = len(set(ptm_no_experimental))                       #This will represent the number of phosphopeptides\n",
    "\n",
    "    no_missing = collapsed_report.dropna(thresh = threshold, subset = samples)                    #Drop sites that have missing values across ANY of the samples \n",
    "    ptm_sites = len(no_missing)\n",
    "\n",
    "    \n",
    "    \n",
    "    ret_df = pd.DataFrame({'Run': sample, '%_Precursors_with_' + ptm_phrase: enrichment,\n",
    "             'num_modified_sequences': num_modified_sequences,\n",
    "             'num_' + ptm_phrase + '_peptides': unique_ptm_peptides,\n",
    "             'num_' + ptm_phrase + '_sites': ptm_sites}, index=[0])\n",
    "\n",
    "    return (ret_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ec933d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_any(full, collapsed_report, sample):\n",
    "    \"\"\"\n",
    "    Generates summary stats for any data frame (could be subset of original report)\n",
    "\n",
    "    Args:\n",
    "        full: Data frame that has not been collapsed\n",
    "        collapsed_report: Collapsed PTM site report\n",
    "        sample: Column name that will be used for the summary document\n",
    "\n",
    "    Returns: Dictionary with values for\n",
    "        % precursors containing target modification\n",
    "        Number of modified peptides: Including different versions of experimentally-introduced modifications and heavy mods\n",
    "        Number of phosphopeptides: Number of unique PTM sequences, NOT including experimentally-introduced modifications, but including heavy mods\n",
    "        Number of phosphosites: Collapsed PTM-sites, including heavy mods\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    num_modified_sequences = len(full[all_sequences].unique())                          #All precursors, including experimentally-introduced mods and heavy mods\n",
    "    \n",
    "    #Enrichment efficiency\n",
    "    ptm_sequences = [x for x in full[all_sequences] if target_ptm in x]                 #Only PTM-containing precursors\n",
    "    enrichment = (len(set(ptm_sequences)) / num_modified_sequences) * 100         #PTM-containing precursors / all precursors\n",
    "    \n",
    "\n",
    "    #Number of unique PTM modified sequences\n",
    "    ptm_no_experimental = list(map(remove_experimental_mods, ptm_sequences))      #Remove experimentally-introduced modifications, keep heavy mods\n",
    "    unique_ptm_peptides = len(set(ptm_no_experimental))                           #This will represent the number of PTM-peptides\n",
    "\n",
    "    #Number of phosphosites per sample\n",
    "    if sample != 'Combined':\n",
    "        select = collapsed_report[sample].values.tolist()\n",
    "        \n",
    "        if engine == 'SN':\n",
    "            res = [i for i in select if i is not None]\n",
    "            \n",
    "        if engine == 'DIANN':\n",
    "            res = [i for i in select if math.isnan(i) == False]\n",
    "        ptm_sites = len(res)\n",
    "\n",
    "    #Length of data frame is the number of phosphosites identified across all samples searched together\n",
    "    if sample == 'Combined':\n",
    "        ptm_sites = len(collapsed_report)\n",
    "        \n",
    "    #Create data frame to add to summary report\n",
    "    ret_df = pd.DataFrame({'Run': sample, '%_Precursors_with_' + ptm_phrase: enrichment,\n",
    "        'num_modified_sequences': num_modified_sequences,\n",
    "        'num_' + ptm_phrase + '_peptides': unique_ptm_peptides,\n",
    "        'num_' + ptm_phrase + '_sites': ptm_sites}, index=[0])\n",
    "    \n",
    "    \n",
    "    return (ret_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "924da384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(report, collapsed_report, samples):\n",
    "    \"\"\"\n",
    "    Reports summary stats across all runs searched together.\n",
    "\n",
    "    Args:\n",
    "        report: Spectronaut Normal Report\n",
    "        collapsed_report: Collapsed PTM site report\n",
    "        samples: List of all file names\n",
    "\n",
    "    Returns: Datafrane with following values for each sample, combined data set, and intersection across samples:\n",
    "        % precursors containing target modification (STY)\n",
    "        Number of modified peptides: Including different versions of experimentally-introduced modifications and heavy mods\n",
    "        Number of phosphopeptides: Number of unique STY sequences, NOT including experimentally-introduced modifications, but including heavy mods\n",
    "        Number of phosphosites: Collapsed phosphosites, including heavy mods\n",
    "\n",
    "    \"\"\"\n",
    "    print('Summarizing data...')\n",
    "    data = pd.DataFrame(columns= ['Run', '%_Precursors_with_' + ptm_phrase, 'num_modified_sequences','num_' + ptm_phrase + '_peptides','num_' + ptm_phrase + '_sites'])       #Initialize datarframe with columns\n",
    "\n",
    "    non_missing = []                                        #Collect precursors found in each sample individually, will be list of sets\n",
    "\n",
    "    for s in samples:\n",
    "        one = report[report[file_col] == s]\n",
    "        ret = summarize_any(one, collapsed_report, s)       #Returns a row of the dataframe containing stats for that sample\n",
    "        data = pd.concat([data, ret], ignore_index=True)\n",
    "\n",
    "        set_IDs = set(one[all_sequences])\n",
    "        non_missing.append(set_IDs)\n",
    "\n",
    "    #Add row for summarized combined data\n",
    "    total = summarize_any(report, collapsed_report, 'Combined')\n",
    "    data = pd.concat([data, total], ignore_index=True)\n",
    "\n",
    "    #Add row for intersection of data across all samples searched together\n",
    "    intersection = summarize_non_missing(non_missing, collapsed_report, samples, 'Intersection',len(samples))\n",
    "    fifty_perc_complete = summarize_non_missing(non_missing, collapsed_report, samples, 'Fifty Percent Complete',len(samples)/2)\n",
    "    \n",
    "    data = pd.concat([data, intersection], ignore_index=True)\n",
    "    data = pd.concat([data, fifty_perc_complete], ignore_index = True)\n",
    "\n",
    "    data.to_csv(wd + 'VMSummary_' + str(threshold) + '.tsv', sep='\\t', index=False)\n",
    "    print('Summarizing complete')\n",
    "    \n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6afb478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary():\n",
    "    \n",
    "    if engine == 'SN':\n",
    "        summarize(report, collapsed_report, samples)\n",
    "        \n",
    "    if engine == 'DIANN':\n",
    "        summarize(localized_report, collapsed_report, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22976562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_flanking(collapsed_report, organism):  #Adapted for both software\n",
    "    \n",
    "    print('Finding Flanks!!!')\n",
    "    \n",
    "    meta = pd.read_csv('Z:\\\\Helium_Tan\\\\PhosphositePlus\\\\Phosphorylation_site_dataset', sep = '\\t')\n",
    "    meta = meta[meta['ORGANISM'] == organism]   #Filter for organism, typically human\n",
    "\n",
    "    dict = {}\n",
    "\n",
    "    for index, row in meta.iterrows():\n",
    "        prot_id = row['ACC_ID']\n",
    "        residue = row['MOD_RSD']\n",
    "        flank_seq = row['SITE_+/-7_AA'].upper()\n",
    "\n",
    "        res = residue.split('-')[0]\n",
    "        k = (prot_id, res)\n",
    "\n",
    "        if k not in dict:\n",
    "            dict[k] = None\n",
    "            dict[k] = flank_seq\n",
    "\n",
    "        else:\n",
    "            dict[k] = flank_seq\n",
    "\n",
    "\n",
    "\n",
    "    print('Done building dictionary. Now appending flanked sequences to collapsed report...')\n",
    "\n",
    "\n",
    "    flanks_column = []         #Global list\n",
    "    \n",
    "    #Iterate through entries and add flanking sequences for each protein ID and site\n",
    "    for index, row in collapsed_report.iterrows():\n",
    "        \n",
    "        prot_id = row[prot_column].split(';')                             #Get all protein IDs a sequence is mapped to\n",
    "        locations = list(row[ptm_phrase +'ProteinLocations'])             #PTM locations in protein\n",
    "\n",
    "        row_flanks = {}                                             #Each row/entry will have a dictionary if there are flanking sequences in phosphosite table, with protein ID as key and flanking sequence(s) as value(s)\n",
    "        \n",
    "        #Find flanks for each protein ID listed in entry\n",
    "        for i in range(0, len(prot_id)):                            \n",
    "            \n",
    "            prot_flanks = []         #If there are multiple sites on the same sequence, they will all be appended here\n",
    "            \n",
    "            sites = locations[i]     #Can contain multiple sites\n",
    "            id = prot_id[i]\n",
    "\n",
    "            contains = False\n",
    "            for s in sites:                                          #Each PTM-site on sequence associated with that protein ID\n",
    "                rep_k = (id,s)                                       #(Protein ID, PTM Location on that protein ID)\n",
    "\n",
    "                if rep_k in dict.keys():                             #Phosphosite table has mapping seq\n",
    "                    contains = True\n",
    "                    prot_flanks.append(dict[rep_k])\n",
    "                    \n",
    "                else:\n",
    "                    prot_flanks.append('')                           #Sometimes if a seq is doubly phosphorylated only one site will be mapped in phosphosite table\n",
    "\n",
    "\n",
    "            if contains:\n",
    "                if id not in row_flanks:\n",
    "                    row_flanks[id] = None\n",
    "                    row_flanks[id] = prot_flanks\n",
    "                else:\n",
    "                    row_flanks[id] = prot_flanks\n",
    "\n",
    "        if len(row_flanks) > 0:\n",
    "            flanks_column.append(row_flanks)\n",
    "        else:\n",
    "            flanks_column.append(None)\n",
    "\n",
    "\n",
    "    collapsed_report['7AA_Flanking'] = flanks_column\n",
    "    collapsed_report.to_csv(wd + 'VM_CollapsedReport_Flanks.tsv', sep = '\\t', index = False)\n",
    "    print('Completed adding flanking sequences.')\n",
    "    \n",
    "    return(collapsed_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c0f8cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging alphamap output and DIA_NN Precursor matrix\n",
      "Summarizing data...\n",
      "Summarizing complete\n"
     ]
    }
   ],
   "source": [
    "collapsed_report = generate_collapsed_report()\n",
    "get_summary()\n",
    "\n",
    "# if ptm_phrase == 'Phospho':\n",
    "#     find_flanking(collapsed_report, \"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphamap",
   "language": "python",
   "name": "alphamap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
